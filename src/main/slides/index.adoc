= Databases
from ancient engineers manuscripts
:idprefix:
:title-slide-background-image: openart-image_4O7jGP_b_1741948237657_raw.png
:title-slide-background-size: cover
:stem: asciimath
:backend: html
:source-highlighter: highlightjs
:revealjs_history: true
:revealjs_theme: night
:revealjs_controls: false
:imagesdir: images
:customcss: css/custom.css
:revealjs_width: 1920
:revealjs_height: 1080
:revealjs_mouseWheel: true

== about me

Neo4j (a graph database) performance engineer

over 20 years with JVM, +
since early days of no native threads and, +
no JIT and slow as hell GC

speaker, coder, architect

=== WARNING

Over years, in my professional career +
I have developed strong affection to + 
programming languages and databases

=== !

image::https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExYmhza2M5dWhrNTFyeWhmd3IwdnljeTJzcTYwamhqNmdybzdsemhrZSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/NAQoNEtra2e7pfCqxG/giphy.gif[background]

=== !

And I tend to get too emotional and excited when I am talking about it :)

=== DISCLAIMER

for the sake of simplicity and entertainment + 
I have ignored some concepts, +
or presented them in a overly naive way

=== !

**THIS IS NOT COMPUTER SCIENCE CLASS**

there will be a list of references +
at the end of presentation +
if you want to go deeper

== what is a database?

[quote,,Wikipedia]
  In computing, a database is an organized collection of data or a type of data store based on the use of a database management system (DBMS), the software that interacts with end users, applications, and the database itself to capture and analyze the data.

=== !

[quote,,Wikipedia]
  Formally, a "database" refers to a set of related data accessed through the use of a "database management system" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. 

[role="highlight_title"]
=== WAT?

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExaGt4NDBoOHhhMmJ1am04bDBrNXVycmVlZjNianl3MnF2ZHlmcmg5YiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/4JVTF9zR9BicshFAb7/giphy.gif[background]

== history of databases

A concept of a database is a as old as humanity +
at the moment we realized that +
we can persist our knowledge +
and use to come up with new concepts and ideas +
this is the moment databases where born

[role="highlight_title"]
=== ancient DBAs at work

image::TheGeniusInnovationThatMadetheGreatLibraryofAlexandriaWork.jpeg[background,size=cover]

== a real reason for database systems to exits

=== we need guarantees

(and of course we can relax some of them)

**A**tomicity +
**I**solation +
**C**onsistency +
**D**urability

=== 640kb is not enough

Database systems solve the problem of working with data that doesn't fit into RAM, +
or even single machine (but it is a different story)

=== RAM is expensive 

**but it is fast**

**(and volatile)**

=== !

* Main memory is about $3.15 per GB (DDR4)
* SSD storage is about $0.10 per GB
* Hard disk storage is about $0.019 per GB

=== !

[quote,,attributed to Pat Helland]
  Database is a cache over event log

=== !

[quote,,Me]
  Database is a an anti-corruption layer for data access patterns

== database management systems

**a quick and dirty guide** +

=== for extremely busy developers

image::https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExODN0enp5ajFzZTZwdTNoY2x3bDUxa2hrcThhNDh3dGRsdHU5ZmNmdyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/U16eJ5dFcfiolA5u85/giphy.gif[background]

=== !

for the sake of this discussion +
we will remove distributed databases from the picture +
and focus on a single database server node

=== in the begning there was a query

image::https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExNWU1OWI2NDRla25naW44cDJzaHNsdGpweHF4eDRqNDZmbTMwa29qaCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/idGw983D7CHrrXa2eO/giphy.gif[background]

=== !

**SELECT users.name** +
**FROM users** +
**WHERE users.age>18 AND users.active=true** +
**ORDER BY users.registration_date LIMIT 10** 

=== the database system

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Database Query") --> B("Query Parsing")
    B --> C("Query Planning")
    C --> D("Query Plan Optimization")
    D --> E("Query Plan Execution")
    E --> F("Lock Manager")
    F --> G("Block Manager")
    E --> H("Transaction Log")
    G --> I("Database Storage")
....

=== !

**SELECT users.name** +
**FROM users** +
**WHERE users.age>18 AND users.active=true** +
**ORDER BY users.registration_date LIMIT 10** 

=== AST tree

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("SQL Query") --> B("SELECT")
    B --> C("users.name")
    A --> D("FROM")
    D --> E("users")
    A --> F("WHERE")
    F --> G("Condition")
    G --> H("users.age > 18")
    G --> I("users.active = true")
    A --> J("ORDER BY")
    J --> K("users.registration_date")
    A --> L("LIMIT")
    L --> M("10")
....

=== logical plan

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Logical Plan") --> B("Scan 'users' Table")
    B --> C("Filter => 'users.age > 18' and 'users.active = true'")
    C --> D("Sort => 'users.registration_date'")
    D --> F("Projection => 'users.name'")
    F --> G("Limit => 10 rows")
....

=== execution plan

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Physical Plan") --> B("Index Range Scan => 'users.age'")
    B --> C("Filter => 'users.active = true'")
    C --> D("Projection => 'users.name'")
    D --> E("Index Sort => 'users.registration_date'")
    E --> F("Limit => 10 rows")
....

=== !

image::https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDFhYTB3bXA2aXgzNTcycmJod2RybHpsOXFreHJhOTBkN3UyaWZocCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Wp0c6wZckiptf9gnow/giphy.gif[background,size=contain]

=== !

I think it was all obvious until this point +
and common tribal knowledge

**QUESTION** +
How these translate into actual data access operations?

=== !

will take a bottom-up approach to explain how things work

=== !

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Database Query") --> B("Query Parsing")
    B --> C("Query Planning")
    C --> D("Query Plan Optimization")
    D --> E("Query Plan Execution")
    E --> F("Lock Manager")
    F --> G("Block Manager")
    E --> H("Transaction Log")
    G --> I("Database Storage")
....

[role="highlight_title"]
== storage

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcWNnbHBqcjAzNGp1ZmkyZTI1MWticHRrNTR2M2dvZWQ2NmpzZzF1dyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/N35rW3vRNeaDC/giphy.gif[background]

=== !

whatever you work with tables, graphs, documents +
which are structured data, +
at the end of the day +
you need to squeeze them into flat, one-dimensional files

=== important factors

* storing entities as fixed size vs variable size records
* storing entities unordered vs ordered collections 
* entities have schema vs schemaless

[role="highlight_title"]
=== praise the machine

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExNGdmNTBxY3RlMHE2ZThsNjJ3ZWhycG94bXFqdG9udDFpanI3YTBhYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/yJwZtUrulZMUXCLZgu/giphy.gif[background]

=== universal laws of computer engineering

**disks are slow (unless you're rich enough to use NVRAM)**

**operating system always read/writes data in fixed size pages**

**CPU and memory controllers find it difficult to chase pointers**

=== unordered files

* sequenced files
* heap files
* ISAM (indexed sequential access method)

=== heap files

* heap files store entities in blocks
* every block contains 1 or more entities
* you add entities to first available and free block, 
* when there is not enough space, new block is allocated

=== !

[mermaid,scale=8,theme=forest]
....
flowchart LR
  subgraph "Heap File"
    direction LR
    subgraph B1["Block 1"]
      direction LR
      R11["Record 1"] --- R12["Record 2"] --- R13["Record 3"]
      R13 --- FS1["Free Space 1KB"]
    end
    subgraph B2["Block 2"] 
      direction LR
      R21["Record 1"] --- R22["Record 2"] --- R23["Record 3"] --- R24["Record 4"]
      R24 --- FS2["Free Space 0.5KB"]
    end
    subgraph B3["Block 3"]
      direction LR
      R31["Record 1"] --- R32["Record 2"]
      R32 --- FS3["Free Space 2KB"]
    end
    subgraph B4["Block 4"]
      direction LR
      FS4["Free Space 4KB"]
    end
    B1 --> B2 --> B3 --> B4
  end
....

=== pros & cons

* fairly easy to implement
* writes are fast, you always append at the end of file
* random access is **SLOW**
* space reclamation is **TRICKY**

=== ordered files

* hash files 
* cluster files
* B+ tree files
* LSM (log structured merge trees)

=== B+ tree

a default storage for indexes in many databases

* The B+ tree is a balanced  m-ary search tree. It follows a multi-level index format.
* In the B+ tree, leaf nodes denote actual data pointers. B+ tree ensures that all leaf nodes remain at the same height.
* In the B+ tree, the leaf nodes are linked using a link list. Therefore, a B+ tree can support random access as well as sequential access.
* In the B+ tree, every leaf node is at equal distance from the root node. The B+ tree is of the order n where n is fixed for every B+ tree.

=== !

* It contains an internal nodes and leaf nodes.
* The leaf node of the B+ tree can contain at least n/2 record pointers and n/2 key values.
* At most, a leaf node contains n record pointer and n key values.
* Every leaf node of the B+ tree contains one block pointer P to point to next leaf node.

=== !

image::https://upload.wikimedia.org/wikipedia/commons/3/37/Bplustree.png[]

=== pros & cons

* O(log n) search, insertion and deletion time complexity
* helpful for query optimization since they may be used to sort data and range queries
* require more space than other types of indexes, which can be a concern for databases with limited storage
* not as efficient for write-heavy workloads, as every update to the index requires a disk write operation

=== best of both worlds

why no always use B+ tree?

imagine that most of the time your entities are bigger than single block?

most of databases use mixture of heap/sequential files with hash/b+tree indexes

[role="highlight_title"]
== block manager

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExcjl0OGZhcWJwMGhqN3g1dzFhbzRreHU4d2h1emZodDhuMTVxN3J6ayZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Yh0qLwfpAogL9vVxhL/giphy.gif[background]

=== !

how to squeeze more data than you have available memory?

you don't always need all the data + 
(we call it liveset)

block manager loads data on demand, when needed +
unloads when data is no longer used

=== !

when query engine needs specific entity + 
row, document, node

it requests it from block manager, +
when block manager doesn't have it memory +
it loads it from disk

=== !

when query engine modifies the entity, +
block manager marks it as "dirty", +
and writes to a storage when needed

for example when block is evicted,
to reclaim memory for another block

=== !

database data is organized into blocks +
data is always read and written as a whole block +
(aka mechanical sympathy)

=== block eviction

it is a set of cache eviction algorithms, like:

* LRU
* LFU
* LIRS (Low Inter-reference Recency Set)
* TinyLFU
* Clock Pro
* ... and others

What we are looking here is a good balance +
between hit ratio and eviction algorithms overhead

[role="highlight_title"]
== locking

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExeDFiZXdtajhkOGNzeHFpNXdyMGNoZnJ4Z3BoaWViNnJ4ZXQzamNiYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/mIvrv5Qe0kHlu/giphy.gif[background]

=== !

This is where things are getting messy, a little bit

what happens when multiple threads are going to write to the same block?

=== locking protocols

single query can modify multiple blocks during its execution

database systems employ techniques called locking protocols +
to efficiently manage locks and also avoid deadlocks +
and what is most important, +
**ensure consistency of our data**

=== Concurrency control protocols

* Lock Based Concurrency Control Protocol
* Time Stamp Concurrency Control Protocol
* Validation Based Concurrency Control Protocol

=== simplistic lock protocol

It is the simplest way of locking the data while transaction. + 
Simplistic lock-based protocols allow all the transactions +
to get the lock on the data before insert or delete or update on it. +
It will unlock the data item after completing the transaction.

=== two-phase locking protocol

For 2PL, the only used data-access locks are read-locks (shared locks) and write-locks (exclusive locks). Below are the rules for read-locks and write-locks:

* A transaction is allowed to read an object if and only if it is holding a read-lock or write-lock on that object.
* A transaction is allowed to write an object if and only if it is holding a write-lock on that object.
* A schedule (i.e., a set of transactions) is allowed to hold multiple locks on the same object simultaneously if and only if none of those locks are write-locks. If a disallowed lock attempts on being held simultaneously, it will be blocked.

=== !

By the 2PL protocol, locks are applied and removed in two phases:

* Expanding phase: locks are acquired and no locks are released.
* Shrinking phase: locks are released and no locks are acquired.

The two phase locking rules can be summarized as: each transaction must never acquire a lock after it has released a lock. 

=== !

What about table level or row level locking ?

They are another level of concurrency control, +
implemented higher in a database systems stack

we call these low-level (block manager) concurrency controls latch, +
and higher-level (like table or row level) locks

=== !

[quote,,What are some best practices for implementing row-level locking?]
  Evaluate the average row size, and based on that number of rows you will have on one page. If you have hundreds of rows on a page stop right here because you won't see any increase throughput. All the contention will just switch from row level to page level and explicit locking will have no positive impact.

[role="highlight_title"]
== transaction log

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExaTJ2cHdhdml3dmE2Z2R5bzRjcGt6dW5nMG8xcHJsc3Izc3Y2aGFvZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/h2IsKmfwNh3I4/giphy.gif[background]

=== !

As you can imagine there can be a situation when, +
transaction is committed, + 
but block manager haven't written all the changes to storage

you may ask, why it doesn't happen on every commit?

[role="highlight_title"]
=== need for speed

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExa3c3cGYzM3NvbnF2NDZzMjY1aHIwYWYxNXZ0dWNwNG1uNTdkbW0zdSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3ZrH1fpTB7H4M3Jk4b/giphy.gif[background]

=== REMEMBER

file system is slow

=== atomicity

there is no way to ensure writing multiple blocks will be atomic, +
operating systems and hardware doesn't provide such guarantees 

=== durability

the fact that you asked operating system to write block of data, +
doesn't mean it is persistent when system call returns, +
because operating system also has a thing called page cache

=== filesystems are databases

=== !

you would have to call `fsync` after every write +
(and most databases have this setting)

(unless you force O_DIRECT mode, +
but this whole another flamewar +
in database and operating systems community)

=== journal file

When a transaction modifies a page, the DBMS copies the original page to a separate journal file before overwriting the master version. After restarting, if a journal file exists, then the DBMS restores it to undo changes from uncommited transactions.

=== shadow pages

when we lock on a block for write, + 
to isolate reads from writes, +
we often use technique called shadow pages +
where we create a copy of a actual block,
and make modifications in a copy, +
when transaction is committed we replace +
original block with newly create block

=== write ahead log

With write-ahead logging, the DBMS records all the changes made to the database in a log file (on stable storage) before the change is made to a disk page. + 
The log contains sufficient information to perform the necessary undo and redo actions to restore the database after a crash. +
The DBMS must write to disk the log file records that correspond to changes made to a database object before it can flush that object to disk

=== !

The DBMS first stages all of a transaction’s log records in volatile storage. All log records pertaining to an updated page are then written to non-volatile storage before the page itself is allowed to be overwritten in non-volatile storage. +
A transaction is not considered committed until all its log records have been written to
stable storage.

=== !

When the transaction starts, write a <BEGIN> record to the log for each transaction to mark its starting point. +
When a transaction finishes, write a <COMMIT> record to the log and make sure all log records are flushed before it returns an acknowledgment to the application. 

=== !

Each log entry contains information about the change to a single object:

* Transaction ID.
* Object ID.
* Before Value (used for UNDO).
* After Value (used for REDO).

=== !

The DBMS must flush all of a transaction’s log entries to disk before it can tell the outside world that a transaction has successfully committed. +
The system can use the “group commit” optimization to batch multiple log flushes together to amortize overhead. + 
The DBMS can write dirty pages to disk whenever it wants as long as it’s after flushing the corresponding log records.

=== transaction state

transaction state has to be applied +
to query results, +
so within the scope of transaction +
you can see your own writes

[role="highlight_title"]
== query engine

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExdTJqY242OGtoYno0ODR4OXlneWQ1cHlwemUxcTdvcHJ0eGdranE4cyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/vf5TjQrio0TBK/giphy.gif[background]

=== !

query is parsed and transformed into a query plan

query engine is responsible for executing query

=== query processing models

Operators are function-like pieces of code +
which take tuples an emit tuples as its result

Operators in the query plan are arranged into a tree.

Typically operators are binary (1–2 children) +
The same query plan can be executed in multiple ways

Data flows from the leaves of this tree towards the root +
The output of the root node in the tree is the result of the query 

=== !

A query processing model defines how the system executes a query plan.

It specifies things like the direction in which the query plan is evaluated and what kind of data is passed between operators along the way. 

There are different models of processing models that have various trade-offs for different workloads.

These models can also be implemented to invoke the operators either from top-to-bottom or from bottom-to-top. Although the top-to-bottom approach is much more common, the bottom-to-top approach can allow for tighter control of caches/registers in pipelines.

=== execution models

The three execution models that we consider are:

* iterator model
* materialization model
* vectorized / batch Model

[role="highlight_title"]
=== WAT?

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExaGt4NDBoOHhhMmJ1am04bDBrNXVycmVlZjNianl3MnF2ZHlmcmg5YiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/4JVTF9zR9BicshFAb7/giphy.gif[background]

[role="highlight_title"]
=== show me the code

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExNDZybHdvNXZ1Mm5tNGt2MDlqM3d1M3ZxaTNrZmtvM21uOWVycDBlOSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xoicctrOv5aGw6mCZi/giphy.gif[background]

=== !

We are going to take a sneak peak +
under the hood of most common +
query processing model, iterator model, +

[role="highlight_title"]
=== know also as volcano model

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExYjk5cnEwODdpZGZucTZocmF6ajc0djFsNHhnbDFoOWo2OHY5bHE1ZiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/dUIedRUfa35QdQNDA3/giphy.gif[background]

=== !

[source,java]
----
interface Operator {
    void open();
    Tuple next();
    void close();
}
----

=== !

[source,java]
----
import java.util.List;
import java.util.Iterator;

class Scan implements Operator {
    private Cursor<Tuple> cursor;
    private Iterator<Tuple> iterator;

    public Scan(Cursor<Tuple> cursor) {
        this.cursor = cursor;
    }

    @Override
    public void open() {
        iterator = cursor.iterator();
    }

    @Override
    public Tuple next() {
        if (iterator.hasNext()) {
            return iterator.next();
        } else {
            return null;
        }
    }

    @Override
    public void close() {
        iterator = null;
    }
}
----

=== !

[source,java]
----
import java.util.function.Predicate;

class Selection implements Operator {
    private Operator child;
    private Predicate<Tuple> predicate;

    public Selection(Operator child, Predicate<Tuple> predicate) {
        this.child = child;
        this.predicate = predicate;
    }

    @Override
    public void open() {
        child.open();
    }

    @Override
    public Tuple next() {
        Tuple tuple;
        while ((tuple = child.next()) != null) {
            if (predicate.test(tuple)) {
                return tuple;
            }
        }
        return null;
    }

    @Override
    public void close() {
        child.close();
    }
}
----

=== !

[source,java]
----
class Projection implements Operator {
    private Operator child;
    private int[] columns;

    public Projection(Operator child, int[] columns) {
        this.child = child;
        this.columns = columns;
    }

    @Override
    public void open() {
        child.open();
    }

    @Override
    public Tuple next() {
        Tuple tuple = child.next();
        if (tuple == null) {
            return null;
        }
        Object[] projectedValues = new Object[columns.length];
        for (int i = 0; i < columns.length; i++) {
            projectedValues[i] = tuple.getValue(columns[i]);
        }
        return new Tuple(projectedValues);
    }

    @Override
    public void close() {
        child.close();
    }
}
----

=== !

[source,java]
----
import java.util.Arrays;
import java.util.List;

class QueryExecution {
    public static void main(String[] args) {
        // Sample data
        List<Tuple> data = Arrays.asList(
            new Tuple(new Object[]{1, "Alice", 15}),
            new Tuple(new Object[]{2, "Bob", 25}),
            new Tuple(new Object[]{3, "Charlie", 35})
        );

        // Scan operator
        Scan scan = new Scan(data);

        // Selection operator (age > 18)
        Selection selection = new Selection(scan, tuple -> ((int) tuple.getValue(2)) > 18);

        // Projection operator (select name)
        Projection projection = new Projection(selection, new int[]{1});

        // Execute query
        projection.open();
        Tuple tuple;
        while ((tuple = projection.next()) != null) {
            System.out.println(Arrays.toString(tuple.values));
        }
        projection.close();
    }
}
----

[role="highlight_title"]
== query planner

image::https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExeWM4ZXIwZzE1dDN1aDJtbjFmbm9zaGhyZzNzcHB4bDFldGtla3RsMCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/61jhLbOyn4x05KP5qf/giphy.gif[background]

=== !

Database system needs to translate a query into an executable query plan. +
But there are different ways to execute each operator in a query plan (e.g., join algorithms, predicates orders) and there will be differences in performance among these plans.

=== is this optimal query plan?

[source,sql]
----
SELECT users.name FROM users WHERE users.age>18 AND users.active=true ORDER BY users.registration_date LIMIT 10 
----

=== !

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Logical Plan") --> B("Scan 'users' Table")
    B --> C("Filter => 'users.age > 18' and 'users.active = true'")
    C --> D("Sort => 'users.registration_date'")
    D --> F("Projection => 'users.name'")
    F --> G("Limit => 10 rows")
....

=== execution plan

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Physical Plan") --> B("Scan 'users' Table")
    B --> C("Filter => 'users.age > 18'")
    C --> D("Filter => 'users.active = true'")
    D --> E("Sort => 'users.registration_date'")
    F --> G("Projection => 'users.name'")
    G --> H("Limit => 10 rows")
....

=== execution plan

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Physical Plan") --> B("Scan 'users' Table")
    B --> C("Filter => 'users.active = true'")
    C --> D("Filter => 'users.age > 18'")
    D --> E("Sort => 'users.registration_date'")
    F --> G("Projection => 'users.name'")
    G --> H("Limit => 10 rows")
....

=== execution plan

[mermaid,height=800,theme=forest]
....
flowchart TD
    A("Physical Plan") --> B("Index Range Scan => 'users.age'")
    B --> C("Filter => 'users.active = true'")
    C --> D("Projection => 'users.name'")
    D --> E("Index Sort => 'users.registration_date'")
    E --> F("Limit => 10 rows")
....

=== query planner

[quote,,]
  The first implementation of a query optimizer was IBM System R and was designed in the 1970s. Prior to this, people did not believe that a DBMS could ever construct a query plan better than a human. Many concepts and design decisions from the System R optimizer are still in use today.

=== who is faster?

* The first approach is to use static rules, or heuristics. Heuristics match portions of the query with known patterns to assemble a plan. These rules transform the query to remove inefficiencies. Although these rules may require consultation of the catalog to understand the structure of the data, they never need to examine the data itself.
* An alternative approach is to use cost-based search to read the data and estimate the cost of executing equivalent plans. The cost model chooses the plan with the lowest cost

=== query costs

* filesystem access
* memory usage for query (aggregating operators, like sort, average)
* network
* CPU

=== !

The planner/optimizer generates a mapping of a logical algebra expression to the optimal equivalent physical algebra expression. +
The logical plan is roughly equivalent to the relational algebra expressions in the query.
Physical operators define a specific execution strategy using an access path for the different operators in the query plan. +
Physical plans may depend on the physical format of the data that is processed (i.e. sorting,
compression).

=== logical plan optimizations

* Perform filters as early as possible (predicate pushdown).
* Reorder predicates so that the DBMS applies the most selective one first.
* Breakup a complex predicate and pushing it down (split conjunctive predicates)
* Perform projections as early as possible to create smaller tuples and reduce intermediate results (projection pushdown).
* Project out all attributes except the ones requested or requires.
* The ordering of JOIN operations is a key determinant of query performance. 

=== physical plan optimizations

* index selection
* join operators implementation
* sorting implementation
* index back projections

=== it is all cardinality

how many tuples are returned by operator? +
you want the most selective operator +
in the beginning of the pipeline

so you need to collect statistical information, +
and use it to prepare most optimal plan

[role="highlight_title"]
=== distributed databases?

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExOWNseXo4cWZqczE0YnkwM3NmaW1hdzI3b3pxdnU0Z3V0ZTNjc2Q4cyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/O5oRZiBtdSqS3K7YnE/giphy.gif[background]

=== references

https://www.db-book.com/[Database System Concepts] +
https://15445.courses.cs.cmu.edu/fall2024/[Introduction to database systems, Andy Pavlo course] +
https://www.databass.dev/[Database Internals: A Deep Dive Into How Distributed Data Systems Work] +
https://radiki.dev/[Chris Gioran (former Neo4j architect) blog about implementing databases] +
https://www.amazon.com/Database-Design-Implementation-Data-Centric-Applications/dp/3030338355[Database Design and Implementation] +
https://www.amazon.com/Concurrency-Control-Transactions-Processing-Systems/dp/3639340248[Concurrency Control in Transactions Processing Systems] +
https://www.amazon.com/Transaction-Processing-Concepts-Techniques-Management[Transaction Processing: Concepts and Techniques] 

[role="highlight_title"]
=== thank you

image::https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExYWQ0YzRwNWFicmsza3lnbXRzdGF2dTc3Y3VxNHlxd3kweTE2YzdjNyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/atOpRKayP1IJ2/giphy.gif[background]